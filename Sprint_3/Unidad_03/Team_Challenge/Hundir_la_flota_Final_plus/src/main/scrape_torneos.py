
import os
import json
import requests
import pandas as pd
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
import streamlit as st

st.set_page_config(page_title="Scraper EDHTop16 x Fecha Real + Progreso", layout="wide")

BASE_URL = "https://edhtop16.com"
TORNEOS_URL = f"{BASE_URL}/tournaments"
MAZOS_DIR = "mazos_completos"
CARTAS_DIR = "cartas_por_mazo"
RESPALDO_DIR = os.path.join(CARTAS_DIR, "respaldos_json")

os.makedirs(MAZOS_DIR, exist_ok=True)
os.makedirs(CARTAS_DIR, exist_ok=True)
os.makedirs(RESPALDO_DIR, exist_ok=True)

st.title("üìÖ Scraping Torneos (√öltimos 90 d√≠as) con Progreso")

def limpiar_nombre(nombre):
    return "".join(c for c in nombre if c.isalnum() or c in "._-").replace(" ", "_")

def obtener_lista_torneos():
    try:
        r = requests.get(TORNEOS_URL, headers={"User-Agent": "Mozilla/5.0"})
        r.raise_for_status()
        soup = BeautifulSoup(r.text, "html.parser")
        torneos = []
        grupos = soup.find_all("div", class_="group")

        for grupo in grupos:
            enlace = grupo.find("a", href=True)
            span = grupo.find("span")
            if not enlace or not span:
                continue
            fecha_texto = span.text.strip()
            try:
                fecha = datetime.strptime(fecha_texto.replace("st", "").replace("nd", "").replace("rd", "").replace("th", ""), "%B %d %Y")
            except:
                continue
            if fecha >= datetime.today() - timedelta(days=90):
                torneos.append({
                    "fecha": fecha,
                    "slug": enlace["href"],
                    "nombre": enlace.text.strip()
                })
        return torneos
    except Exception as e:
        st.error(f"‚ùå Error al obtener lista de torneos: {e}")
        return []

def obtener_html_torneo(slug):
    try:
        r = requests.get(BASE_URL + slug, headers={"User-Agent": "Mozilla/5.0"})
        r.raise_for_status()
        return r.text
    except:
        return ""

def extraer_mazos(html):
    soup = BeautifulSoup(html, "html.parser")
    enlaces = soup.select("a[href*='moxfield.com/decks']")
    mazos = []
    for link in enlaces:
        contenedor = link.find_parent("div", class_="group")
        if not contenedor:
            continue
        jugador = link.text.strip("ü•áü•àü•â ").strip()
        url_mazo = link["href"]
        comandante = contenedor.find_all("a")[1].text.strip()
        spans = contenedor.find_all("span")
        posicion = spans[0].text.strip() if len(spans) > 0 else ""
        resultado = spans[1].text.strip() if len(spans) > 1 else ""
        wins = losses = draws = 0
        for part in resultado.split("/"):
            if "Wins" in part:
                wins = int(part.split(":")[1].strip())
            elif "Losses" in part:
                losses = int(part.split(":")[1].strip())
            elif "Draws" in part:
                draws = int(part.split(":")[1].strip())
        mazos.append({
            "Jugador": jugador,
            "Comandante": comandante,
            "Posici√≥n": posicion,
            "Wins": wins,
            "Losses": losses,
            "Draws": draws,
            "URL Mazo": url_mazo
        })
    return mazos

def verificar_mazo_publico(url):
    deck_id = url.split("/")[-1]
    api_url = f"https://api2.moxfield.com/v2/decks/all/{deck_id}"
    try:
        response = requests.get(api_url, headers={"User-Agent": "Mozilla/5.0"})
        if response.status_code == 403:
            return False, deck_id, None
        response.raise_for_status()
        return True, deck_id, response.json()
    except:
        return False, deck_id, None

def guardar_cartas(deck_id, data):
    cartas = []
    for zona, cartas_zona in data.get("boards", {}).items():
        for carta_id, carta_data in cartas_zona.items():
            nombre = carta_data.get("card", {}).get("name", "desconocido")
            if not nombre:
                continue
            cartas.append({
                "zona": zona,
                "nombre": nombre,
                "cantidad": carta_data.get("quantity", 1),
                "isCommander": carta_data.get("isCommander", False)
            })
    with open(os.path.join(CARTAS_DIR, f"{deck_id}.json"), "w", encoding="utf-8") as f:
        json.dump(cartas, f, indent=2, ensure_ascii=False)
    with open(os.path.join(RESPALDO_DIR, f"{deck_id}_raw.json"), "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    return cartas

if st.button("üöÄ Iniciar scraping de torneos"):
    torneos = obtener_lista_torneos()
    total_torneos = len(torneos)
    st.info(f"üìÖ Torneos encontrados: {total_torneos}")

    progress_bar = st.progress(0)
    text_bar = st.empty()

    total_mazos = 0
    total_guardados = 0

    for i, torneo in enumerate(torneos):
        html = obtener_html_torneo(torneo["slug"])
        mazos = extraer_mazos(html)
        for mazo in mazos:
            total_mazos += 1
            es_publico, deck_id, data = verificar_mazo_publico(mazo["URL Mazo"])
            if es_publico:
                guardar_cartas(deck_id, data)
                jugador = limpiar_nombre(mazo["Jugador"])
                comandante = limpiar_nombre(mazo["Comandante"])
                nombre_csv = f"{jugador}_{comandante}.csv"
                df = pd.DataFrame([mazo])
                df.to_csv(os.path.join(MAZOS_DIR, nombre_csv), index=False)
                total_guardados += 1

        progress = int((i + 1) / total_torneos * 100)
        progress_bar.progress(progress)
        text_bar.markdown(f"‚è≥ Procesando torneo {i + 1} / {total_torneos}  ‚Äî  Mazos p√∫blicos: {total_guardados} / {total_mazos}")

    st.success(f"‚úÖ Scraping completado: {total_guardados} mazos p√∫blicos descargados de {total_mazos} detectados.")
